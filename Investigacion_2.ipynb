{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "based-subcommittee",
   "metadata": {},
   "source": [
    "# Investigación 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-neighborhood",
   "metadata": {},
   "source": [
    "## Tareas previas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-victor",
   "metadata": {},
   "source": [
    "### Selección de conjunto de datos\n",
    "\n",
    "Para esta investigación se empleará el conjunto de datos obtenido en la investigación 1 tras procesar el juego de datos original. La investigación 1 se puede consultar en https://github.com/eliecer9000/investigacion-1-reconocimiento-patrones. Se procedió a exportar el conjunto de datos resultante por medio del método del marco de datos 'to_csv'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solar-table",
   "metadata": {},
   "source": [
    "### Métricas de evaluación\n",
    "\n",
    "Estudiaremos algunas métricas empleadas para evaluar el desempeño de los algoritmos empleados para clasificación y regresión.\n",
    "\n",
    "#### Clasificación\n",
    "\n",
    "Antes de iniciar con las métricas más comunes es necesario describir brevemente un concepto muy empleado: la matriz de confusión.\n",
    "\n",
    "##### Matriz de confusión\n",
    "\n",
    "Esta matriz provee una manera de contrastar los resultados que predice el modelo contra los resultados de referencia.\n",
    "\n",
    "<img src=\"./conf_matrix.png\" width=40%, height=40%>\n",
    "<h5><center>Figura 1. Matriz de confusión</h5></center>\n",
    "\n",
    "La figura 1 presenta un ejemplo de esta matriz. Nótese que el título general de las columnas es \"Referencia\", mientras que el título general de las filas es \"Predicción\".\n",
    "Dentro de la matriz de confusión se presentan cuatro conceptos clave, éstos son: falso positivo, falso negativo, verdadero positivo y verdadero negativo.\n",
    "\n",
    "###### Falso positivo (FP)\n",
    "\n",
    "Corresponde a la cantidad de predicciones que el modelo de clasificación categorizó como positivo (\"Sí\" en el caso de la figura 1) y según los valores de referencia, no lo era. En el caso de la figura 1, esto corresponde con 60 instancias.\n",
    "\n",
    "###### Falso negativo (FN)\n",
    "\n",
    "Corresponde a la cantidad de predicciones que el modelo de clasificación categorizó como negativo (\"No\" en el caso de la figura 1) y según los valores de referencia, no lo era. En el caso de la figura 1, esto corresponde con 40 instancias.\n",
    "\n",
    "###### Verdadero positivo (TP)\n",
    "\n",
    "Corresponde a la cantidad de predicciones que el modelo de clasificación categorizó como positivo (sí en el caso de la figura 1) y según los valores de referencia, están clasificados de esta manera. En el caso de la figura 1, esto corresponde con 460 instancias.\n",
    "\n",
    "###### Verdadero negativo (TN)\n",
    "\n",
    "Corresponde a la cantidad de predicciones que el modelo de clasificación categorizó como negativo (no en el caso de la figura 1) y según los valores de referencia, están clasificados de esta manera. En el caso de la figura 1, esto corresponde con 440 instancias.\n",
    "\n",
    "##### Exactitud (accuracy)\n",
    "\n",
    "Esta métrica se obtiene al sumar la cantidad de predicciones correctas del modelo y dividirlo por la cantidad de instancias.\n",
    "\n",
    "$$\n",
    "Exactitud = \\frac{TP + TN}{n} * 100\\%\n",
    "$$\n",
    "\n",
    "\n",
    "Por ejemplo: Tomando la matriz de confusión de la figura 1, se tiene que:\n",
    "\n",
    "$$\n",
    "Exactitud = \\frac{440 + 460}{1000} * 100\\% = 90\\%\n",
    "$$\n",
    "\n",
    "##### Precisión (precision)\n",
    "\n",
    "Existen casos en los que la exactitud no muestra correctamente el desempeño del modelo de clasificación. Esto se da cuando las instancias no se encuentran balanceadas según las clases. Por ejemplo, si se tiene un conjutno de datos para clasificar si una imagen corresponde con un gato o no, y se tienen 900 imágenes que son gatos y 100 que no lo son. Si el modelo predijese que todas las instancias son gatos, se tendría una exactitud del $90\\%.$\n",
    "Esto no tiene mucho sentido pues el modelo no está mostrando un comportamiento correcto al detectar las imágenes que no corresponden a gatos, aunque parece que lo está haciendo bien.\n",
    "\n",
    "La precisión considera la proporción de la predicción verdadera (positiva o negativa) respecto a la suma de los verdaderos y falsos (positivos o negativos según corresponda). Es decir:\n",
    "\n",
    "$$\n",
    "Precisión = \\frac{Verdadero}{Verdadero + Falso} * 100\\%\n",
    "$$\n",
    "\n",
    "Al emplear esta métrica con los datos de la Figura 1 asumiendo que describen la clasificación binaria de gatos y no gatos, se tiene que:\n",
    "\n",
    "$$\n",
    "Precisión_{gatos} = \\frac{460}{460 + 60} * 100\\% = 88.46\\%\n",
    "$$\n",
    "\n",
    "$$\n",
    "Precisión_{no-gatos} = \\frac{440}{40 + 440} * 100\\% = 91.67\\%\n",
    "$$\n",
    "\n",
    "##### Sensibilidad (recall)\n",
    "\n",
    "Es la proporción de los verdaderos positivos respecto a todos los positivos en el conjunto de datos.\n",
    "\n",
    "$$\n",
    "Sensibilidad = \\frac{TP}{TP + FN} * 100\\%\n",
    "$$\n",
    "\n",
    "Al usar los datos de la figura 1, asumiendo que esta describe la clasificación binaria de gatos y no gatos en el conjunto de datos:\n",
    "\n",
    "$$\n",
    "Sensibilidad_{gatos} = \\frac{460}{460 + 40} * 100\\% = 92\\%\n",
    "$$\n",
    "\n",
    "##### Especificidad\n",
    "\n",
    "Si se calcula la misma razón, pero para los casos negativos, se denomina especificidad.\n",
    "\n",
    "$$\n",
    "Especificidad = \\frac{TN}{TN + FP} * 100\\%\n",
    "$$\n",
    "\n",
    "Al usar los datos de la figura 1, asumiendo que esta describe la clasificación binaria de gatos y no gatos en el conjunto de datos:\n",
    "\n",
    "$$\n",
    "Especifidad_{no-gatos} = \\frac{440}{440 + 60} * 100\\% = 88\\%\n",
    "$$\n",
    "\n",
    "##### Puntuación F1 (F1 score)\n",
    "\n",
    "Es posible que se desee dar prioridad a la precisión sobre la sensibilidad. Sin embargo, la puntuación F1 considera ambas métricas.\n",
    "Esta se describe matemáticamente como:\n",
    "\n",
    "$$\n",
    "F_1 = \\frac {2 * Precision * Sensibilidad}{Precision + Sensibilidad}\n",
    "$$\n",
    "\n",
    "Por lo que empleando los datos de la figura 1, tenemos:\n",
    "\n",
    "$$\n",
    "F_1 = \\frac {2 * 88.46 * 92}{88.46 + 92} = 90.2\\%\n",
    "$$\n",
    "\n",
    "#### Regresión\n",
    "\n",
    "En el caso que el modelo se esté empleando para obtener una proyección de un valor contínuo, estamos hablando de modelos de regresión. Estos modelos se pueden evaluar con métricas como el error cuadrático medio y el error absoluto promedio.\n",
    "\n",
    "##### Error cuadrático medio (MSE)\n",
    "\n",
    "Este error se obtiene al calcular el cuadrado de la diferencia entre el valor esperado y el valor obtenido del modelo. Finalmente, se pondera dividiéndolo entre la cantidad de instancias.\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{n} \\sum_{i=0}^n(y_i - \\tilde{y_i})^2\n",
    "$$\n",
    "\n",
    "Así, por ejemplo, si nuestro modelo predice los datos $[15150, 300, 7542, 9584]$ y los datos esperados son $[15100, 302, 7535, 9575]$\n",
    "\n",
    "El MSE viene dado por:\n",
    "$$\n",
    "MSE = \\frac{1}{4} * \\left((15150-15100)^2 + (300-302)^2 + (7542-7535)^2 + (9584-9575)^2\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "MSE = 658.5\n",
    "$$\n",
    "\n",
    "##### Error absoluto medio (MAE)\n",
    "\n",
    "Este error se obtiene al calcular el valor absoluto de la diferencia entre el valor esperado y el valor obtenido del modelo. Finalmente, se pondera dividiéndolo entre la cantidad de instancias.\n",
    "\n",
    "$$\n",
    "MAE = \\frac{1}{n} \\sum_{i=0}^n|y_i - \\tilde{y_i}|\n",
    "$$\n",
    "\n",
    "Así, por ejemplo, si nuestro modelo predice los datos $[15150, 300, 7542, 9584]$ y los datos esperados son $[15100, 302, 7535, 9575]$\n",
    "\n",
    "El MAE viene dado por:\n",
    "$$\n",
    "MAE = \\frac{1}{4} * \\left( |15150-15100| + |300-302| + |7542-7535| + |9584-9575| \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "MAE = 17\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floating-gabriel",
   "metadata": {},
   "source": [
    "### Validación cruzada\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-internship",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "\n",
    "1) https://towardsdatascience.com/metrics-to-evaluate-your-machine-learning-algorithm-f10ba6e38234\n",
    "2) https://towardsdatascience.com/20-popular-machine-learning-metrics-part-1-classification-regression-evaluation-metrics-1ca3e282a2ce\n",
    "3) https://www.tutorialspoint.com/machine_learning_with_python/machine_learning_algorithms_performance_metrics.htm\n",
    "4) https://iopscience.iop.org/article/10.1088/1757-899X/263/4/042087/pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
